---
title: FastDFS笔记
date: 2020-09-07 15:22:42
permalink: /pages/da78ea/
categories:
  - 其它
  - 存储
---
# FastDFS

[toc]

FastDFS 是一个开源的、轻量级、分布式文件系统，它解决了大数据量存储和负载均衡问题，特别适合以中小（建议在：4k-500M）文件为载体的在线服务。

## 基础入门

### 文件系统发展

**单机时代：**

文件和代码存放在一起；

优点：

- 文件访问比较便利；
- 项目直接引用文件；
- 实现起来简单；
- 无需复杂技术；
- 保存、访问文件都很方便；

缺点：

- 文件和代码耦合在一起，文件越多存放越混乱；
- 如果流量比较大，静态文件访问会占据一定的资源（操作系统上下文切换、磁盘I/O），影响正常业务进行，不利于网站快速发展；

**独立文件服务器：**

文件服务器和代码服务器分离；

优点：

- 应用服务器可以更专注发挥动态处理的能力；
- 独立存储，更方便做扩容、容灾和数据迁移；
- 方便做资源请求的负载均衡；
- 方便应用缓存策略（HTTP Header、Proxy Cache等）；
- 方便迁移到 CDN；

缺点：

- 单机存在性能瓶颈；
- 容灾、垂直扩展性稍差

**分布式文件系统：**

优点：

- 扩展能力强；
- 高可用性: 高可用性包含两层，一是整个文件系统的可用性，二是数据的完整和一致性；
- 弹性存储: 可以根据业务需要灵活地增加或缩减数据存储、增删存储池中的资源，不需要中断系统运行。

缺点：系统复杂度稍高，需要更多服务器。

### 分布式存储解决方案对比

![文件系统对比](https://gitee.com/leixiaoai/markdown/raw/master/03.其它/01.存储/images/fastdfs/文件系统对比.png)

### FastDFS 简介

FastDFS 是用 C 语言编写的一款开源的、轻量级分布式文件系统，功能主要有：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等。FastDFS 为互联网量身定制，充分考虑了冗余备份、负载均衡、线性扩容等机制、注重高可用、高性能等指标，使用FastDFS很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务;

**特性：**

- 分组存储（同一组数据内容相同），采用对等结构，不存在单点；
- 文件不分块存储，上传的文件和 OS 文件系统中的文件一一对应；
- 文件 ID 由 FastDFS 生成，作为文件访问凭证，FastDFS 不需要传统的命名服务器；
- 和流行的 web server 无缝衔接，FastDFS 已提供 apache 和 nginx 扩展模块；
- 中、小文件均可以很好支持，支持海量小文件存储；
- 支持多块磁盘，支持单盘数据恢复；
- 支持相同内容的文件只保存一份，节约磁盘空间；
- 支持在线扩容、主从文件；
- 存储服务器上可以保存文件属性（meta-data），V2.0 网络通信采用libevent，支持大并发访问，整体性能更好；
- 下载文件支持多线程方式，支持断点续传；

### FastDFS 构成

![fast-dfs组成](https://gitee.com/leixiaoai/markdown/raw/master/03.其它/01.存储/images/fastdfs/fast-dfs组成.png)

FastDFS 由客户端（Client）、 跟踪服务器（Tracker Server）和存储服务器（Storage Server）构成；

**客户端（client）：**

客户端作为业务请求的发起方，使用 TCP/IP 协议与跟踪器服务器或存储节点进行数据交互；

**跟踪器（tracker）：**

跟踪器的作用是负载均衡和调度，在文件上传时根据一些策略找到存储节点，提供文件上传服务，跟踪器在访问上起负载均衡的作用，可以随时增加或下线而不会影响线上服务；

**存储节点（storage）：**

存储节点作用是存储文件，客户端上传的文件最终存储在Storage服务器上，存储节点没有实现自己的文件系统，而是利用操作系统的文件系统来管理文件，存储节点中的服务器也可以随时增加、下线而不会影响线上服务；

### Linux 安装

参考：<https://github.com/happyfish100/fastdfs/wiki>

- 1.安装编译环境：``yum install git gcc gcc-c++ make automake vim wget libevent -y``；

- 2.安装libfastcommon 基础库：

  ```shell
    mkdir /root/fastdfs
    cd /root/fastdfs
    git clone https://github.com/happyfish100/libfastcommon.git --depth 1
    cd libfastcommon/
    ./make.sh && ./make.sh install
  ```

- 3.安装 FastDFS：

  ```shell
    cd /root/fastdfs
    wget https://github.com/happyfish100/fastdfs/archive/V5.11.tar.gz
    tar -zxvf V5.11.tar.gz
    cd fastdfs-5.11
    ./make.sh && ./make.sh install
    #配置文件准备
    cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf
    cp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf
    cp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf
    cp /root/fastdfs/fastdfs-5.11/conf/http.conf /etc/fdfs
    cp /root/fastdfs/fastdfs-5.11/conf/mime.types /etc/fdfs
  ```

  ```shell
    vim /etc/fdfs/tracker.conf
    #需要修改的内容如下
    port=22122
    base_path=/home/fastdfs
  ```

  ```shell
    vim /etc/fdfs/storage.conf
    #需要修改的内容如下
    port=23000
    base_path=/home/fastdfs # 数据和日志文件存储根目录
    store_path0=/home/fastdfs # 第一个存储目录
    tracker_server=192.168.33.128:22122
    # http访问文件的端口(默认8888,和nginx中保持一致)
    http.server_port=8888
  ```

- 4.启动：

  ```shell
    mkdir /home/fastdfs -p
    /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart
    /usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart
    # 查看所有运行的端口
    netstat -ntlp
  ```

- 5.测试上传：

  ```shell
    vim /etc/fdfs/client.conf
    #需要修改的内容如下
    base_path=/home/fastdfs
    tracker_server=192.168.33.128:22122

    #测试,返回ID表示成功 如：group1/M00/00/00/11733455.png
    /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /root/fastdfs/11733455.png
    # 返回 group1/M00/00/00/wKghgF9V_dCAEwWJAABZKJkyS3c543.jpg
  ```

- 6.安装fastdfs-nginx-module：

  ![fastdfs-nginx-module](https://gitee.com/leixiaoai/markdown/raw/master/03.其它/01.存储/images/fastdfs/fastdfs-nginx-module.png)

  ```shell
    cd /root/fastdfs
    wget https://github.com/happyfish100/fastdfs-nginx-module/archive/V1.20.tar.gz
    # 解压
    tar -xvf V1.20.tar.gz
    cd fastdfs-nginx-module-1.20/src
    vim config
    # 修改
    ngx_module_incs="/usr/include/fastdfs /usr/include/fastcommon/"
    CORE_INCS="$CORE_INCS /usr/include/fastdfs /usr/include/fastcommon/"
  ```

  ```shell
    cp mod_fastdfs.conf /etc/fdfs/
  ```

  ```shell
    vim /etc/fdfs/mod_fastdfs.conf
    # 需要修改的内容如下
    tracker_server=192.168.33.128:22122
    url_have_group_name=true
    store_path0=/home/fastdfs
  ```

  ```shell
    mkdir -p /var/temp/nginx/client
  ```

- 7.安装nginx：

  ```shell
    cd /root/fastdfs
    wget http://nginx.org/download/nginx-1.15.6.tar.gz
    tar -zxvf nginx-1.15.6.tar.gz
    cd nginx-1.15.6/

    # 添加fastdfs-nginx-module模块
    yum -y install pcre-devel openssl openssl-devel
    ./configure --add-module=/root/fastdfs/fastdfs-nginx-module-1.20/src

    # 编译安装
    make && make install
    # 查看模块是否安装上
    /usr/local/nginx/sbin/nginx -V
  ```

  ```shell
    vim /usr/local/nginx/conf/nginx.conf
    # 添加如下配置
    server {
      listen 8888;
      server_name localhost;
      location ~/group[0-9]/ {
        ngx_fastdfs_module;
      }
    }
  ```

  ```shell
    /usr/local/nginx/sbin/nginx
  ```

- 8.测试下载：

  ```shell
    # 关闭防火墙
    systemctl stop firewalld
    http://192.168.33.128:8888/group1/M00/00/00/wKghgF9V_dCAEwWJAABZKJkyS3c543.jpg
  ```

### java 访问FastDFS

参考： <https://github.com/happyfish100/fastdfs-client-java>

- 1.建立 maven 工程、引入jar包：

  ```xml
    <!--fastdfs的java客户端-->
    <dependency>
      <groupId>cn.bestwu</groupId>
      <artifactId>fastdfs-client-java</artifactId>
      <version>1.27</version>
    </dependency>
  ```

- 2.引入配置文件（fastdfs-client.properties）：

  ```javascript
    fastdfs.connect_timeout_in_seconds = 5
    fastdfs.network_timeout_in_seconds = 30
    fastdfs.charset = UTF-8
    fastdfs.tracker_servers = 192.168.33.128:22122
  ```

- 3.测试代码

  ```java
    import org.csource.common.NameValuePair;
    import org.csource.fastdfs.*;
    import org.junit.jupiter.api.Test;
    import java.io.File;
    import java.io.FileOutputStream;
    public class FastDFSTest {

        // 上传
        @Test
        public void testUpload() {
            try {
                //加载配置文件
                ClientGlobal.initByProperties("fastdfs-client.properties");
                //创建tracker客户端
                TrackerClient tc = new TrackerClient();
                //根据tracker客户端创建连接 获取到跟踪服务器对象
                TrackerServer ts = tc.getConnection();
                StorageServer ss = null;
                //定义storage客户端
                StorageClient1 client = new StorageClient1(ts, ss);
                //文件元信息
                NameValuePair[] list = new NameValuePair[1];
                list[0] = new NameValuePair("fileName", "test.jpg");
                // 上传，返回fileId
                String fileId = client.upload_file1("C:\\Users\\ievan\\Pictures\\test.jpg", "jpg", list);
                System.out.println(fileId);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        // 查询文件信息
        @Test
        public void testQuery() {
            try {
                //加载配置文件
                ClientGlobal.initByProperties("fastdfs-client.properties");
                // 创建tracker客户端
                TrackerClient tc = new TrackerClient();
                // 根据tracker客户端创建连接 获取到跟踪服务器对象
                TrackerServer ts = tc.getConnection();
                StorageServer ss = null;
                //定义storage客户端
                StorageClient1 client = new StorageClient1(ts, ss);
                // 查询文件信息
                FileInfo fileInfo = client.query_file_info1("group1/M00/00/00/wKghgF9WFQeAbFelAADnm4mDiTA437.jpg");
                // 元数据在 get_metadata1 中
                System.out.println(fileInfo);
            } catch (Exception e) {
                e.printStackTrace();
            }
        }

        // 文件下载
        @Test
        public void testDownload() {
            try {
                //加载配置文件
                ClientGlobal.initByProperties("fastdfs-client.properties");
                // 创建tracker客户端
                TrackerClient tc = new TrackerClient();
                // 根据tracker客户端创建连接
                TrackerServer ts = tc.getConnection();
                StorageServer ss = null;
                // 定义storage客户端
                StorageClient1 client = new StorageClient1(ts, ss);
                // 下载
                byte[] bs = client.download_file1("group1/M00/00/00/wKghgF9WFQeAbFelAADnm4mDiTA437.jpg");
                FileOutputStream fos = new FileOutputStream(new File("xxxx.jpg"));
                fos.write(bs);
                fos.close();
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }
  ```

## 系统架构和功能原理

### 架构详解

![fast-dfs组成](https://gitee.com/leixiaoai/markdown/raw/master/03.其它/01.存储/images/fastdfs/fast-dfs组成.png)

**storage server：** 存储服务器（存储节点或数据服务器），文件、文件属性（meta data）都保存到存储服务器上，Storage server 直接利用 OS 的文件系统调用管理文件。

Storage server（后简称storage）以组（group或volume）为单位组织，一个 group 内包含多台 storage 机器，数据互为备份，存储空间以group内容量最小的storage为准，所以建议group内的多个storage尽量配置相同，以免造成存储空间的浪费。

以 group 为单位能方便的进行应用隔离、负载均衡、副本数定制（比如：将不同应用数据存到不同的group 就能隔离应用数据，同时还可根据应用的访问特性来将应用分配到不同的 group 来做负载均衡）；

缺点是 group 的容量受单机存储容量的限制（建议同一组容量相同，否则以最小为准），同时当 group 内有机器坏掉时，数据恢复只能依赖 group 内的其它机器，使得恢复时间会很长。

group 内每个 storage 的存储依赖于本地文件系统，storage 可配置多个数据存储目录，比如有10块磁盘，分别挂载在 /data/disk1 ~ /data/disk10 ，则可将这 10 个目录都配置为 storage 的数据存储目录，storage 接受到写文件请求时，会根据配置好的规则，选择其中一个目录存储文件。为了避免单个目录下的文件数太多，在 storage 第一次启动时，会在每个数据存储目录里创建2 级子目录，默认每级 256 个，总共 65536 个文件，新写的文件会以 hash 的方式被路由到其中某个子目录下，然后将文件数据直接作为一个本地文件存储到该目录中。

**group：** 组，也可称为卷。同组内服务器上的文件是完全相同的 ，同一组内的 storage server 之间是对等的，文件上传、删除等操作可以在任意一台 storage server 上进行，操作完成后由同步线程完成同步；

**meta data：** 文件相关属性，键值对（Key Value Pair）方式，如：``width: 1080``；

**tracker server：** 跟踪服务器，主要做调度、负载均衡的作用。在内存中记录集群中所有存储组和存储服务器的状态，因为不记录文件索引信息，所以占用的内存量很少。每个 storage 在启动后会连接 Tracker，告知自己所属的 group 等信息，并保持周期性的心跳，tracker 根据 storage 的心跳信息，建立 group==>[storage server list] 的映射表，Tracker 管理的元信息很少，全部存储在内存中。另外 tracker 上的元信息都是由 storage 汇报生成的，本身不需要持久化任何数据，这样使得 tracker 非常容易扩展，直接增加 tracker 机器即可扩展为 tracker cluster 来服务，cluster 里每个 tracker 可以理解为对等的，所有的 tracker 都接受 storage 的心跳信息，生成元数据信息来提供读写服务；

**client：** 客户端，作为业务请求的发起方，通过专有接口，使用TCP/IP协议与跟踪器服务器或存储节点进行数据交互。FastDFS 向使用者提供基本文件访问接口，比如 upload、download、append、delete、断点续传等，以客户端库的方式提供给用户使用；

### 设计理念

**轻量级：**

- Tracker server 在内存中记录分组、Storage server 的状态等信息，不记录文件索引信息，占用的内存量很少。客户端（应用）和 Storage server 访问 Tracker server 时， Tracker server 扫描内存中的分组和 Storage server 状态信息，然后给出应答，Tracker server 非常轻量化，不会成为系统瓶颈；

- FastDFS 中的 Storage server 直接利用 OS 的文件系统管理文件。FastDFS 不会对文件进行分块存储，客户端上传的文件和 Storage server 上的文件一一对应。对于互联网应用，文件分块存储没有多大的必要，它没有带来多大的好处，反而增加了系统的复杂性，与支持文件分块存储的 DFS 相比，更加简洁高效，并且完全能满足绝大多数互联网应用的实际需要；

- 在 FastDFS 中，客户端上传文件时，文件 ID 不是由客户端指定，而是由 Storage server 生成后返回给客户端的，文件 ID 中包含了组名、文件相对路径和文件名，Storage server 可以根据文件 ID 直接定位到文件，因此 FastDFS 集群中根本不需要存储文件索引信息。而其他文件系统则需要存储文件索引信息，这样的角色通常称作 NameServer，比如 mogileFS 采用 MySQL 数据库来存储文件索引以及系统相关的信息，MySQL 将成为整个系统的瓶颈；

- FastDFS 轻量级的另外一个体现是代码量较小。最新的 V2.0 包括了 C 客户端 API 、 FastDHT 客户端 API 和 PHP extension 等，代码行数不到 5.2 万行；

**分组存储：**

FastDFS 采用了分组存储方式。集群由一个或多个组构成，集群存储总容量为集群中所有组的存储容量之和，一个组由一台或多台存储服务器组成，同组内的多台 Storage server 之间是对等的互备关系。文件上传、下载、删除等操作可以在组内任意一台 Storage server 上进行（一个组的存储容量为该组存储服务器容量最小的那个，组内存储服务器的容量最好是一致的）。用分组存储方式的好处是灵活、可控性较强，比如上传文件时，可以由客户端直接指定上传的组。一个分组的存储服务器访问压力较大时，可以在该组增加存储服务器来扩充服务能力（纵向扩容），当系统容量不足时，可以增加组来扩充存储容量（横向扩容），采用这样的分组存储方式，可以使用 FastDFS 对文件进行管理，使用主流的 Web server 如 Apache、nginx 等进行文件下载；

**对等结构：**

FastDFS 集群中的 Tracker server 也可以有多台，Tracker server 和 Storage server 均不存在单点问题。Tracker server 之间是可以理解为对等关系，组内的 Storage server 之间也是对等关系，传统的 Master-Slave 结构中的 Master 是单点，写操作仅针对 Master。如果 Master 崩溃，需要将 Slave 提升为 Master，实现逻辑会比较复杂，和 Master-Slave 结构相比，对等结构中所有结点的地位是相同，每个结点都是 Master，不存在单点问题；

### FastDFS 功能原理

#### 文件上传

**文件上传流程：**

![文件上传流程](https://gitee.com/leixiaoai/markdown/raw/master/03.其它/01.存储/images/fastdfs/文件上传流程.png)

- 1.storage server 会定时提交信息给 tracker server，tracker server 把这些信息存储到内存中；
- 2.客户端向 tracker server 发起文件上传请求；
- 3.tracker 在内存中查找可用的 storage；
- 4.tracker 给客户端返回一个可用的 storage 信息；
- 5.客户端接收到 storage 信息后，进行文件上传的请求（发送给 storage）；
- 6.storage 生成 filedId；
- 7.storage 将文件内容写入磁盘；
- 8.返回 fileId 给客户端；
- 9.客户端根据返回的路径做相关处理，一般存放到数据库；

**文件上传内部原理：**

- **1、选择 tracker server 和 group。** 当集群中不止一个 tracker server 时，tracker 之间关系完全对等，客户端在 upload 时可以任意选择一个 trakcer（可用使用 Nginx 负载均衡）。当 tracker 接收到 upload_file 的请求后，接着完成组的选择，在配置文件中修改 store_lookup 选择 group ：
  - 1 表示Round robin，所有的 group 间轮询；
  - 2 表示Specified group，指定某一个确定的 group；
  - 3 表示Load balance，剩余存储空间最大的 group 优先；

- **2、选择 storage server。** 当选定 group 后，tracker 会在 group 内选择一个 storage server 给客户端，使用 配置文件中的 store_server 选择 storage ：
  - 1 表示Round robin，在 group 内的所有 storage 间轮询；
  - 2 表示First server ordered by ip，按 ip 排序；
  - 3 表示First server ordered by priority，按优先级排序（优先级在 storage 上配置）；

- **3、选择storage path。** 当分配好 storage server 后，客户端将向 storage 发送写文件请求，storage 将会为文件分配一个数据存储目录 storage server 可以有多个存放文件的存储路径（可以理解为多个磁盘），store_path*（可用配置多个） 支持规则如下：
  - 0 表示Round robin，多个存储目录间轮询；
  - 2 表示剩余存储空间最多的优先；

- **4、生成文件名。** 选定存储目录之后，storage 会为文件生一个文件名，由 storage server ip、文件创建时间、文件大小、文件 crc32（判断文件是否冗余） 和一个随机数拼接而成，然后将这个二进制串进行 base64 编码，转换为可打印的字符串。当选定存储目录之后，storage 会为文件分配一个文件名，每个存储目录下有两级 256*256 的子目录，storage 会按文件 fileid 进行两次 hash，路由到其中一个子目录，然后将文件以这个文件标示为文件名存储到该子目录下；

- **5、返回文件 id。** 当文件存储到某个子目录后，即认为该文件存储成功，接下来会为该文件返回一个文件id，由group、存储目录、两级子目录、内部文件名、文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成，以 ``group1/M00/00/00/wKghgF9V_dCAEwWJAABZKJkyS3c543.jpg``为例：
  - 组名：文件上传后所在的存储组名称，在文件上传成功后有存储服务器返回，需要客户端自行保存（``group1``）；
  - 虚拟磁盘路径：存储服务器配置的虚拟路径，与磁盘选项 store_path* 对应（``M00``）；
  - 数据两级目录：存储服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据文件（``/00/00``）；
  - 文件名：包含源存储服务器IP地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息（``wKghgF9V_dCAEwWJAABZKJkyS3c543.jpg``）；

#### 文件下载

**下载流程：**

客户端带上文件名请求 Tracker 服务获取到存储服务器的 ip 地址和端口，然后客户端根据返回的 IP 地址和端口号请求下载文件，存储服务器接收到请求后返回文件给客户端。

![文件下载](https://gitee.com/leixiaoai/markdown/raw/master/03.其它/01.存储/images/fastdfs/文件下载.png)

**原理：**

和 upload 一样，在 download 时客户端可以选择任意 tracker server，客户端发送 download 请求给某个 tracker，必须带上文件名，tracker 从文件名中解析出文件的 group、大小、创建时间等信息，然后为该请求选择一个 storage 用来服务读请求。配置文件中 download_server 规则如下:

- 0 表示轮询方式，可以下载当前文件的任意一个 storage server 进行轮询；
- 1 表示哪个为源 storage server 就用哪个；

由于 group 内的文件同步时在后台异步进行的，所以有可能出现在读到时候，文件还没有同步到某些 storage server ,为了尽量避免访问到这样的 storage，需要对应的文件同步规则。

#### 文件同步

**文件同步原理：**

写文件时，客户端将文件写到 group 中的一个 storage server 即认为写文件成功，写完文件后，由后台线程将文件同步到同 group 内的其他 storage server。storage 写文件后，同时会写一份 binlog 文件，binlog 里不包含文件数据，只包含文件名等元信息，这份 binlog 用于后台同步，storage 会记录向 group 内其他 storage 同步的进度，以便重启后能接上次的进度继续同步，进度以时间戳的方式记录，所以要保证集群内所有 server 的时钟保持同步。storage 的同步进度会作为元数据的一部分汇报到 tracker，tracker 在选择 storage 的时候会以同步进度作为参考。比如：一个 group 内有 A、B、C 三个 storage server，A 向 C 同步到 T1 时刻 ，B 向 C 同步到 T2（T2 > T1)，tracker 接收到这些同步进度信息时，就会进行整理，将最小的那个做为 C 的同步时间戳，本例中 T1 为 C 的同步时间戳（T1 以前写的数据都已经同步到 C 上了，不需要重新同步了）。同理，tracker 会为 A、B 生成一个同步时间戳。

**tracker 选择 group 内可用的 storage 的规则：**

- 1.首先上传的源头 storage，源头 storage 只要存活着，肯定包含这个文件，源头的地址被编码在文件名中；
- 2.``文件创建时间戳 == storage 被同步到的时间戳`` 且 ``(当前时间 - 文件创建时间戳) > 文件同步最大时间（如5分钟)`` 文件创建后，认为经过最大同步时间后，肯定已经同步到其他 storage 了；
- 3.``文件创建时间戳 < storage 被同步到的时间戳``，也就是同步时间戳之前的文件，已经被同步了或者当前 storage 是源头；
- 4.``(当前时间 - 文件创建时间戳) > 同步延迟阀值``，经过同步延迟阈值时间，认为文件肯定已经同步了；

#### 文件删除

删除处理流程与文件下载类似：

- 1.Client 询问 Tracker server 可以删除指定文件的 Storage server，参数为文件 ID（包含组名和文件名）；
- 2.Tracker server 返回一台可用的 Storage server；
- 3.Client 直接和该 Storage server 建立连接，完成文件删除；

文件删除 API：delete_file1；

#### 断点续传

提供 appender file 的支持，通过 upload_appender_file 接口完成，appender file 允许在创建后，对该文件进行 append 操作。实际上，appender file 与普通文件的存储方式是相同的，不同的是，appender file 不能被合并存储到 trunk file（合并存储）。续传涉及到的文件大小和 MD5 不会改变（靠 MD5 定位）。续传流程与文件上传类似，先定位到源 storage，完成完整或部分上传，再通过 binlog 进行同 group 内 server 文件同步。

断点续传的 API :upload_appender_file

#### 文件HTTP访问支持

FastDFS 的 tracker 和 storage 都内置了 http 协议的支持，客户端可以通过 http 协议来下载文件，tracker 在接收到请求时，通过 http 的 redirect 机制将请求重定向至文件所在的 storage 上。除了内置的 http 协议外，FastDFS 还提供了通过 apache 或nginx 扩展模块下载文件的支持。

![文件支持Http](https://gitee.com/leixiaoai/markdown/raw/master/03.其它/01.存储/images/fastdfs/文件支持Http.png)

## 集群和配置优化

192.168.22.163|192.168.33.138|192.168.22.40|
-|-|-|
tracker|tracker|tracker|
storage（group1）|storage（group1）|storage（group2）|
nginx|nginx|nginx|

### 集群架构详细配置

- 1.配置 tracker 集群（三台相同）

  ```shell
    vi /etc/fdfs/tracker.conf
    store_lookup=0 # 0是轮询，1是指定组,2是剩余存储空间多的group优先
  ```

- 2.配置 storage 集群

  ```shell
    vi /etc/fdfs/storage.conf
    # 配置如下
    tracker_server=192.168.22.163:22122
    tracker_server=192.168.33.138:22122
    tracker_server=192.168.22.40:22122
    group_name=group1 # 注意组名 192.168.22.40 配置是 group2
    port=23000 # storage 的端口号，同一个组的 storage 端口号必须相同
    # 启动
    /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart
    /usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart
  ```

- 3.查看 storage 的日志查看 tracker 集群信息：

  ```shell
    cat /home/fastdfs/logs/storaged.log
  ```

  访问 FastDFS 时可以把 Tracker 理解成对等的，但实际底层多个 Tracker 时在运行过程中会选择其中一个作为 Leader，Leader 执行一些唯一性的操作。在早期版本中 Tracker-Leader 有两个作用：为新加入的 Storage 分配一个源 Storage、为开启合并存储的 Group 选择 Trunk-Server。但是在最新的版本中实际上只有第二个作用，也就是选择 Trunk-Server。如果连接不上，关闭防火墙 ``systemctl stop firewalld``。

- 4.查看存储集群信息：

  ```shell
    /usr/bin/fdfs_monitor /etc/fdfs/storage.conf
  ```

- 5.测试上传：

  ```shell
    vim /etc/fdfs/client.conf
    # 配置
    tracker_server=192.168.22.163:22122
    tracker_server=192.168.33.138:22122
    tracker_server=192.168.22.40:22122
    # 测试
    /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /root/fastdfs/1.png
    find / -name *.png
  ```

- 6.使用 Nginx 和 FastDFS 集群结合

  ```shell
    vi /etc/fdfs/mod_fastdfs.conf
    # 配置
    tracker_server=192.168.22.163:22122
    tracker_server=192.168.33.138:22122
    tracker_server=192.168.22.40:22122
    group_name=group1 #注意组名 如果是group2 则一定要改

    vim /usr/local/nginx/conf/nginx.conf
    # 添加如下配置
    server {
      listen 8888;
      server_name localhost;
      location ~/group[0-9]/ {
        ngx_fastdfs_module;
      }
    }
    # 执行
    /usr/local/nginx/sbin/nginx
  ```

- 7.测试文件服务器
  <http://192.168.22.163/group1/M00/00/00/wKjIZVyLMi6AH08jAADtXa53YW0605.png>
  <http://192.168.22.40/group1/M00/00/00/wKjIZVyLMi6AH08jAADtXa53YW0605.png>

### FastDFS配置优化

**最大连接数设置：**

  ```shell
    # 配置文件：tracker.conf 和 storage.conf
    # 参数名：max_connections（最多连接数）
    # 缺省值：256
    # 说明：FastDFS 为一个连接分配一个 task buffer，为了提升分配效率，FastDFS采用内存池的做法。
    # FastDFS 老版本直接初始 max_connections 个 buffer ，这个做法显然不是太合理，在 max_connections 设置过大的情况下太浪费内存。v5.04 对预分配采用增量方式，tracker 一次预分配 1024 个，storage 一次预分配 256 个。

    # 在源码中 #define ALLOC_CONNECTIONS_ONCE 1024

    # 总的 task buffer 初始内存占用情况测算如下：
    #   改进前：max_connections * buffer_size
    #   改进后：Min（max_connections，预分配的连接） * buffer_size
    #   使用 v5.04 及后续版本，可以根据实际需要将 max_connections 设置为一个较大的数值，比如 10240 甚至更大；
    #   注意：此时需要将一个进程允许打开的最大文件数（一个连接对应一个文件），调到超过 max_connections，否则 FastDFS server 启动时会报错；
    vi /etc/security/limits.conf # 需要重启系统才能生效
    * soft nofile 65535
    * hard nofile 65535
    # 另外，对于 32 位系统，请注意使用到的内存不要超过 3GB（32位最多4G，需要给操作系统；留 1G）
  ```

**工作线程数设置：**

  ```shell
    # 配置文件：tracker.conf 和 storage.conf
    # 参数名： work_threads
    # 缺省值：4
    # 说明：为了避免CPU上下文切换的开销，以及不必要的资源消耗，不建议将本参数设置得过大。为了发挥出多个 CPU 的效能，系统中的线程数总和，应等于 CPU 总数。

    # 对于 tracker server，公式为：
    #   work_threads = CPU 数 - 1
    # 对于 storage，公式为：
    #   work_threads = CPU 数 - 1 - (disk_reader_threads + disk_writer_threads) * store_path_count
    #   disk_reader_threads 读磁盘线程个数、disk_writer_threads 写磁盘线程个数、store_path_count 磁盘数（可能有多个磁盘），参数设置见：storage磁盘读写线程设置
  ```

**storage目录数设置：**

  ```shell
    # 配置文件： storage.conf
    # 参数名：subdir_count_per_path（每个目录的平均子目录个数）
    # 缺省值：256
    # 说明：FastDFS 采用二级目录的做法，目录会在 FastDFS 初始化时自动创建。如果打开了 trunk 存储方式（use_trunk_file），存储海量小文件的情况下，建议将本参数适当改小，比如设置为32，此时存放文件的目录数为 32 * 32 =1024。假如 trunk 文件大小（trunk_file_size）采用缺省值 64MB，磁盘空间为 2TB，那么每个目录下存放的 trunk 文件数均值为：2TB/(1024 * 64MB) = 32 个
  ```

**storage磁盘读写线程设置：**

  ```shell
    # 配置文件：storage.conf
    #   参数名: disk_rw_separated：磁盘读写是否分离
    #   参数名: disk_reader_threads：单个磁盘读线程数
    #   参数名: disk_writer_threads：单个磁盘写线程数
    # 如果磁盘读写混合，单个磁盘读写线程数为读线程数和写线程数之和（对于单盘挂载方式，磁盘读写线程分别设置为 1 即可）；
    # 如果磁盘做了 RAID（磁盘阵列），那么需要酌情加大读写线程数，这样才能最大程度地发挥磁盘性能；
  ```

**storage 同步延迟相关设置:**

  ```shell
    # 配置文件： storage.conf
    # 参数名: sync_binlog_buff_interval：将 binlog buffer写入磁盘的时间间隔，取值大于0，缺省值为60s；
    # 参数名: sync_wait_msec：如果没有需要同步的文件，对 binlog 进行轮询的时间间隔，取值大于0，缺省值为200ms；
    # 参数名: sync_interval：同步完一个文件后，休眠的毫秒数，缺省值为 0；
    # 为了缩短文件同步时间，可以将上述 3 个参数适当调小即可
  ```

## SpringBoot 项目实战

见 gitee。
